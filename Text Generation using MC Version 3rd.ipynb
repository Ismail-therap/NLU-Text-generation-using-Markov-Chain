{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy in word level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the measure of uncertainty or it's providnig the information about the whole system. The minimum entropy = 0 and maximum is log(k), where k is the number of levels or class have in the system. So, as an example if I have two different category then the minimum entropy is = 0 and in log_2(2) = 1.\n",
    "\n",
    "The maximum value of entropy is logk, where k is the number of categories you are using. Its numeric value will naturally depend on the base of logarithms you are using.\n",
    "\n",
    "Using base 2 logarithms as an example, as in the question: log21 is 0 and log22 is 1, so a result greater than 1 is definitely wrong if the number of categories is 1 or 2. A value greater than 1 will be wrong if it exceeds log2k.\n",
    "\n",
    "In view of this it is fairly common to scale entropy by logk, so that results then do fall between 0 and 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Entropy of the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculating the entropy the define the predictibility fromt the text. The entropy more close to 1 it less predictable or more random. So, the prediction would be better for the text where the entropy is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the word level or charecter level entropy!!!\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def Entropy_of_text(content):\n",
    "    # The data are the tokens of the sentences\n",
    "    \n",
    "    data = content.split(' ')\n",
    "\n",
    "    base =len(data)\n",
    "\n",
    "    if len(data) <= 1:\n",
    "        return 0\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for d in data:\n",
    "        counts[d] += 1\n",
    "\n",
    "    ent = 0\n",
    "\n",
    "    probs = [float(c) / len(data) for c in counts.values()]\n",
    "    for p in probs:\n",
    "        if p > 0.:\n",
    "            ent -= p * math.log(p, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "# Output will be always withing 0 to 1 (we are using the log base = the number of class to make the output in 0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "\n",
    "def create_ngrams(tokens,n):\n",
    "    sequences = [tokens[i:] for i in range(n)]\n",
    "    ngrams = [' '.join(ngram) for ngram in list(zip(*sequences))]\n",
    "    return ngrams, list(set(ngrams))\n",
    "\n",
    "\n",
    "def create_word_mapping(values_list):\n",
    "        values_list.append('<| unknown |>')\n",
    "        value2ind = {value: ind for ind, value in enumerate(values_list)}\n",
    "        ind2value = dict(enumerate(values_list))\n",
    "        return value2ind, ind2value\n",
    "\n",
    "def create_transition_prob_matrix(input_text,n):\n",
    "    \n",
    "    # n is for selecting the ngram\n",
    "    import re\n",
    "\n",
    "    punctuation_pad = '!?.,:-;'\n",
    "    punctuation_remove = '\"()_\\n'\n",
    "\n",
    "\n",
    "    content_preprocess = re.sub(r'(\\S)(\\n)(\\S)', r'\\1 \\2 \\3', input_text)\n",
    "    # Removing charecter like \\n\\n\n",
    "    content_preprocess = content_preprocess.translate(str.maketrans('', '', punctuation_remove))\n",
    "    # Considering , . as different charecter to be used as a state in the transition probability matri\n",
    "    content_preprocess = content_preprocess.translate(str.maketrans({key: ' {0} '.format(key) for key in punctuation_pad}))\n",
    "    # Removing extra space between words or charecters\n",
    "    content_preprocess = re.sub(' +', ' ', content_preprocess)\n",
    "    # remove leading and trailing whitespaces\n",
    "    content = content_preprocess.strip()\n",
    "    \n",
    "    tokens = content.split(' ')\n",
    "    token_dist = list(set(tokens))\n",
    "    \n",
    "\n",
    "    \n",
    "    ngram, ngram_dist = create_ngrams(tokens,n)\n",
    "    \n",
    "    token2ind, ind2token = create_word_mapping(token_dist) # distinct token\n",
    "    ngram2ind, ind2ngram = create_word_mapping(ngram_dist) # distinct ngram\n",
    "    \n",
    "    #tokens_ind = [token2ind[token] if token in token2ind.keys() else token2ind['<| unknown |>']\n",
    "     #                      for token in tokens]\n",
    "\n",
    "    \n",
    "    \n",
    "    row_ind, col_ind, values = [], [], []\n",
    "\n",
    "    for i in range(len(tokens[:-n])):\n",
    "        ngram = ' '.join(tokens[i:i + n])\n",
    "        ngram_ind = ngram2ind[ngram]\n",
    "        next_word_ind = token2ind[tokens[i + n]]\n",
    "\n",
    "        row_ind.extend([ngram_ind])\n",
    "        col_ind.extend([next_word_ind])\n",
    "        values.extend([1])\n",
    "\n",
    "    S = scipy.sparse.coo_matrix((values, (row_ind, col_ind)), shape=(len(ngram2ind), len(token2ind)))\n",
    "    \n",
    "    P = normalize(S, norm='l1', axis=1)\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def powr(P,order):\n",
    "    import numpy as np\n",
    "    k = len(P)\n",
    "    M = np.identity(k)\n",
    "    for i in range(order):\n",
    "        M = np.dot(M,P)\n",
    "    return M\n",
    "\n",
    "\n",
    "def transition_probability_matrix(input_text,n,order):\n",
    "    if order == 1:\n",
    "        P = create_transition_prob_matrix(input_text,n)\n",
    "    else:\n",
    "        P = create_transition_prob_matrix(input_text,n)\n",
    "        P = P.toarray()\n",
    "        P = powr(P,order)\n",
    "        P = scipy.sparse.coo_matrix(P)\n",
    "        P = P.tocsr()\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(input_text):\n",
    "    # n is for selecting the ngram\n",
    "    import re\n",
    "\n",
    "    punctuation_pad = '!?.,:-;'\n",
    "    punctuation_remove = '\"()_\\n'\n",
    "\n",
    "\n",
    "    content_preprocess = re.sub(r'(\\S)(\\n)(\\S)', r'\\1 \\2 \\3', input_text)\n",
    "    # Removing charecter like \\n\\n\n",
    "    content_preprocess = content_preprocess.translate(str.maketrans('', '', punctuation_remove))\n",
    "    # Considering , . as different charecter to be used as a state in the transition probability matri\n",
    "    content_preprocess = content_preprocess.translate(str.maketrans({key: ' {0} '.format(key) for key in punctuation_pad}))\n",
    "    # Removing extra space between words or charecters\n",
    "    content_preprocess = re.sub(' +', ' ', content_preprocess)\n",
    "    # remove leading and trailing whitespaces\n",
    "    content = content_preprocess.strip()\n",
    "    return content\n",
    "    \n",
    "\n",
    "\n",
    "def get_ngram(input_text,n):\n",
    "\n",
    "    content = pre_process(input_text)    \n",
    "    tokens = content.split(' ')\n",
    "    token_dist = list(set(tokens))\n",
    "    \n",
    "    sequences = [tokens[i:] for i in range(n)]\n",
    "    ngrams = [' '.join(ngram) for ngram in list(zip(*sequences))]\n",
    "    \n",
    "    \n",
    "    #ngram, ngram_dist = create_ngrams(tokens,n)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def check_prefix(input_text,prefix,n):\n",
    "    prefix_list = prefix.split(' ')[-n:]\n",
    "    ngrams = get_ngram(input_text,n)\n",
    "    if len(prefix_list) < n:\n",
    "        warnings.warn(\n",
    "                'Prefix is too short, please provide prefix of length: %d. Random ngram used instead.' % n)\n",
    "        return np.random.choice(ngrams)\n",
    "    else:\n",
    "        prefix = ' '.join(prefix_list)\n",
    "        if prefix in ngrams:\n",
    "            return prefix\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                    'Prefix is not included in ngrams of the model. Provide another prefix. Random ngram used instead.')\n",
    "            return np.random.choice(ngrams)\n",
    "\n",
    "        \n",
    "def return_next_word(input_text,prefix,n):\n",
    "    prefix = check_prefix(input_text,prefix,n)\n",
    "    content = pre_process(input_text)\n",
    " \n",
    "    \n",
    "    tokens = content.split(' ')\n",
    "    token_dist = list(set(tokens))\n",
    "    \n",
    "\n",
    "    \n",
    "    ngram, ngram_dist = create_ngrams(tokens,n)\n",
    "    token2ind, ind2token = create_word_mapping(token_dist) # distinct token\n",
    "    ngram2ind, ind2ngram = create_word_mapping(ngram_dist) # distinct ngram\n",
    "    \n",
    "    # Transition prob matrix\n",
    "    transition_matrix_prob = create_transition_prob_matrix(input_text,n)\n",
    "    #transition_matrix_prob = transition_probability_matrix(input_text,n,order)\n",
    "    prefix_ind = ngram2ind[prefix]\n",
    "    weights = transition_matrix_prob[prefix_ind].toarray()[0]\n",
    "    token_ind = np.random.choice(range(len(weights)), p=weights)\n",
    "    next_word = ind2token[token_ind]\n",
    "    return next_word\n",
    "\n",
    "def reverse_preprocess(text):\n",
    "    text_reverse = re.sub(r'\\s+([!?\"\\'().,;-])', r'\\1', text)\n",
    "    text_reverse = re.sub(' +', ' ', text_reverse)\n",
    "    return text_reverse\n",
    "\n",
    "\n",
    "\n",
    "def generate_sequence(input_text,prefix,n, k):\n",
    "    prefix = check_prefix(input_text,prefix,n)\n",
    "    sequence = prefix.split(' ')\n",
    "\n",
    "    for i in range(k):\n",
    "        next_word = return_next_word(input_text,prefix,n)\n",
    "        sequence.append(next_word)\n",
    "        prefix = ' '.join(sequence[-n:])\n",
    "\n",
    "    return reverse_preprocess(' '.join(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game of throns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5713295213655606"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'E:/NMT MS/Spring 22/MATH 486/Project/game_of_thrones/game_of_thrones.txt'\n",
    "input_text = open(path, 'r', encoding='utf-8').read()\n",
    "Entropy_of_text(content = input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my mother says I'll kill,\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = number of ngram consider ... n= 1 means only individual word used, 2 means it will create bigram and then create the word\n",
    "# k = number of word or character need to be generated.\n",
    "\n",
    "generate_sequence(input_text,prefix = \"my\",n = 1,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Philosopher's Stone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6543526856202017"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"E:/NMT MS/Spring 22/MATH 486/Project/Harry Potter/Book 1 - The Philosopher's Stone.txt\"\n",
    "input_text = open(path, 'r', encoding='utf-8').read()\n",
    "Entropy_of_text(content = input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my toad, boiled potatoes,'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(input_text,prefix = \"my\",n = 1,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macbeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7103438046543191"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"E:/NMT MS/Spring 22/MATH 486/Project/Books_EngFr/Books_EngFr/English/shakespeare/Macbeth.txt\"\n",
    "input_text = open(path, 'r', encoding='utf-8').read()\n",
    "Entropy_of_text(content = input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my Prayers with horrorsDirenesse familiar to'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(input_text,prefix = \"my\",n = 1,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
